name: Build Vanilla GAN Model
description: Builds Vanilla GAN model using master config with fully connected architecture
inputs:
  - name: gan_config_json
    type: String
    description: Master GAN configuration as JSON string
  - name: model_name
    type: String
    description: Model name (vanilla_gan)
outputs:
  - name: model_out
    type: Model
  - name: gan_config_base64_updated
    type: String
    description: Updated master GAN configuration (base64 encoded)
  - name: model_info
    type: String

implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
      - sh
      - -c
      - |
        pip install torchvision==0.15.2 --quiet
        echo "Torchvision installed"
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import sys
        import json
        import pickle
        import base64
        import traceback
        import time
        import importlib.util
        
        parser = argparse.ArgumentParser(description='Vanilla GAN Model Builder')
        parser.add_argument('--gan_config_json', type=str, required=True, help='Master GAN config as JSON string')
        parser.add_argument('--model_name', type=str, required=True, help='Model name (vanilla_gan)')
        parser.add_argument('--model_out', type=str, required=True, help='Output path for model')
        parser.add_argument('--gan_config_base64_updated', type=str, required=True, help='Output path for updated config (base64)')
        parser.add_argument('--model_info', type=str, required=True, help='Output path for model info')
        args = parser.parse_args()
        
        print("VANILLA GAN MODEL BUILDER STARTING")
        print("="*60)
        print(f"gan_config_json length: {len(args.gan_config_json)}")
        print(f"model_name: {args.model_name}")
        
        # Parse JSON config
        try:
            gan_config = json.loads(args.gan_config_json)
            print("Master GAN config parsed successfully")
        except Exception as e:
            print(f"Failed to parse JSON config: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        model_config = gan_config.get('model', {})
        
        # Force model type to vanilla_gan
        model_config['gan_type'] = 'vanilla_gan'
        model_type = 'vanilla_gan'
        print(f"Building {model_type.upper()} model")
        
        # Import torch
        try:
            import torch
            import torch.nn as nn
            import torch.nn.functional as F
            print(f"PyTorch version: {torch.__version__}")
            print(f"CUDA available: {torch.cuda.is_available()}")
        except ImportError as e:
            print(f"Failed to import torch: {e}")
            sys.exit(1)
        
        # Define Vanilla GAN architecture directly
        class VanillaGenerator(nn.Module):
            def __init__(self, latent_dim, output_dim, hidden_layers=[256, 512, 1024]):
                super(VanillaGenerator, self).__init__()
                self.latent_dim = latent_dim
                self.output_dim = output_dim
                
                layers = []
                input_dim = latent_dim
                
                # Hidden layers
                for hidden_dim in hidden_layers:
                    layers.append(nn.Linear(input_dim, hidden_dim))
                    layers.append(nn.ReLU())
                    input_dim = hidden_dim
                
                # Output layer
                layers.append(nn.Linear(input_dim, output_dim))
                layers.append(nn.Tanh())  # Output in range [-1, 1]
                
                self.model = nn.Sequential(*layers)
            
            def forward(self, z):
                return self.model(z)
        
        class VanillaDiscriminator(nn.Module):
            def __init__(self, input_dim, hidden_layers=[1024, 512, 256]):
                super(VanillaDiscriminator, self).__init__()
                self.input_dim = input_dim
                
                layers = []
                current_dim = input_dim
                
                # Hidden layers
                for hidden_dim in hidden_layers:
                    layers.append(nn.Linear(current_dim, hidden_dim))
                    layers.append(nn.LeakyReLU(0.2))
                    layers.append(nn.Dropout(0.3))
                    current_dim = hidden_dim
                
                # Output layer
                layers.append(nn.Linear(current_dim, 1))
                layers.append(nn.Sigmoid())  # Output probability
                
                self.model = nn.Sequential(*layers)
            
            def forward(self, x):
                return self.model(x)
        
        # =============================================================================
        # FORWARD-FORWARD DISCRIMINATOR IMPLEMENTATION
        # =============================================================================
        
        class ForwardForwardDiscriminatorBlock(nn.Module):
            def __init__(self, input_dim, output_dim, config):
                super(ForwardForwardDiscriminatorBlock, self).__init__()
                self.config = config
                
                # Main transformation
                self.linear = nn.Linear(input_dim, output_dim)
                self.activation = nn.LeakyReLU(0.2)
                self.dropout = nn.Dropout(config.get('discriminator_dropout', 0.3))
                
                # Local predictor for this block
                self.local_predictor = nn.Sequential(
                    nn.Linear(output_dim, 128),
                    nn.ReLU(),
                    nn.Linear(128, 1),
                    nn.Sigmoid()
                )
            
            def forward(self, x):
                x = self.linear(x)
                x = self.activation(x)
                x = self.dropout(x)
                return x
            
            def compute_goodness(self, x):
                # Goodness based on squared activations (power/activity level)
                return torch.sum(x**2, dim=-1)
            
            def forward_forward_loss(self, pos_goodness, neg_goodness):
                theta = self.config.get('ff_theta', 2.0)
                pos_margin = self.config.get('ff_positive_margin', 2.0)
                neg_margin = self.config.get('ff_negative_margin', 0.0)
                
                pos_loss = torch.log(1 + torch.exp(-(pos_goodness - theta - pos_margin)))
                neg_loss = torch.log(1 + torch.exp(neg_goodness - theta + neg_margin))
                
                return pos_loss.mean() + neg_loss.mean()
        
        class ForwardForwardDiscriminator(nn.Module):
            def __init__(self, input_dim, hidden_layers, config):
                super(ForwardForwardDiscriminator, self).__init__()
                self.config = config
                self.ff_trained = False
                
                # Create FF blocks
                self.blocks = nn.ModuleList()
                current_dim = input_dim
                
                for hidden_dim in hidden_layers:
                    block = ForwardForwardDiscriminatorBlock(current_dim, hidden_dim, config)
                    self.blocks.append(block)
                    current_dim = hidden_dim
                
                # Final classification layer
                self.final_layer = nn.Sequential(
                    nn.Linear(current_dim, 1),
                    nn.Sigmoid()
                )
            
            def forward(self, x, return_layers=False):
                x = x.view(x.size(0), -1)
                
                if return_layers:
                    layer_outputs = []
                
                h = x
                for block in self.blocks:
                    h = block(h)
                    if return_layers:
                        layer_outputs.append(h)
                
                output = self.final_layer(h).view(-1)
                
                if return_layers:
                    return output, layer_outputs
                return output
            
            def forward_single_block(self, x, block_idx):
                x = x.view(x.size(0), -1)
                
                h = x
                for i in range(block_idx + 1):
                    h = self.blocks[i](h)
                return h
        
        # =============================================================================
        # CAFO DISCRIMINATOR IMPLEMENTATION
        # =============================================================================
        
        class CAFODiscriminatorBlock(nn.Module):
            def __init__(self, input_dim, output_dim, config):
                super(CAFODiscriminatorBlock, self).__init__()
                self.config = config
                
                # Main transformation
                self.linear = nn.Linear(input_dim, output_dim)
                self.activation = nn.LeakyReLU(0.2)
                self.dropout = nn.Dropout(config.get('discriminator_dropout', 0.3))
                
                # Local predictor for this block
                self.local_predictor = nn.Sequential(
                    nn.Linear(output_dim, 64),
                    nn.ReLU(),
                    nn.Linear(64, 1),
                    nn.Sigmoid()
                )
            
            def forward(self, x):
                x = self.linear(x)
                x = self.activation(x)
                x = self.dropout(x)
                return x
        
        class CAFODiscriminator(nn.Module):
            def __init__(self, input_dim, hidden_layers, config):
                super(CAFODiscriminator, self).__init__()
                self.config = config
                self.cafo_trained = False
                
                # Create CAFO blocks
                self.blocks = nn.ModuleList()
                current_dim = input_dim
                
                for hidden_dim in hidden_layers:
                    block = CAFODiscriminatorBlock(current_dim, hidden_dim, config)
                    self.blocks.append(block)
                    current_dim = hidden_dim
                
                # Final classification layer
                self.final_layer = nn.Sequential(
                    nn.Linear(current_dim, 1),
                    nn.Sigmoid()
                )
            
            def forward(self, x):
                x = x.view(x.size(0), -1)
                
                h = x
                for block in self.blocks:
                    h = block(h)
                
                return self.final_layer(h).view(-1)
        
        class VanillaGANConfig:
            def __init__(self, input_dim=784, latent_dim=100, generator_layers=[256, 512, 1024], 
                         discriminator_layers=[1024, 512, 256], lr=0.0002, batch_size=64, 
                         epochs=200, device='cpu', **kwargs):
                self.input_dim = input_dim
                self.latent_dim = latent_dim
                self.generator_layers = generator_layers
                self.discriminator_layers = discriminator_layers
                self.lr = lr
                self.batch_size = batch_size
                self.epochs = epochs
                self.device = device
                
                # Additional config items
                for key, value in kwargs.items():
                    setattr(self, key, value)
        
        class VanillaGAN:
            def __init__(self, config):
                self.config = config
                self.device = torch.device(config.device)
                
                # Create generator
                self.generator = VanillaGenerator(
                    config.latent_dim, 
                    config.input_dim, 
                    config.generator_layers
                ).to(self.device)
                
                # Create discriminator based on algorithm
                algorithm = getattr(config, 'training_algorithm', 'backprop')
                if algorithm == 'forward_forward':
                    self.discriminator = ForwardForwardDiscriminator(
                        config.input_dim, 
                        config.discriminator_layers,
                        config.__dict__
                    ).to(self.device)
                    print(f"Created Forward-Forward discriminator with {len(config.discriminator_layers)} blocks")
                elif algorithm == 'cafo':
                    self.discriminator = CAFODiscriminator(
                        config.input_dim, 
                        config.discriminator_layers,
                        config.__dict__
                    ).to(self.device)
                    print(f"Created CAFO discriminator with {len(config.discriminator_layers)} blocks")
                else:
                    self.discriminator = VanillaDiscriminator(
                        config.input_dim, 
                        config.discriminator_layers
                    ).to(self.device)
                    print("Created standard Vanilla discriminator")
                
                # Initialize weights
                self._initialize_weights()
                
                # Optimizers
                self.g_optimizer = torch.optim.Adam(
                    self.generator.parameters(), 
                    lr=config.lr, 
                    betas=(0.5, 0.999)
                )
                self.d_optimizer = torch.optim.Adam(
                    self.discriminator.parameters(), 
                    lr=config.lr, 
                    betas=(0.5, 0.999)
                )
                
                # Loss function
                self.criterion = nn.BCELoss()
                
                print(f"Vanilla GAN initialized:")
                print(f"  Generator: {sum(p.numel() for p in self.generator.parameters())} parameters")
                print(f"  Discriminator: {sum(p.numel() for p in self.discriminator.parameters())} parameters")
                print(f"  Device: {self.device}")
            
            def _initialize_weights(self):
                for module in [self.generator, self.discriminator]:
                    for m in module.modules():
                        if isinstance(m, nn.Linear):
                            nn.init.normal_(m.weight.data, 0.0, 0.02)
                            if m.bias is not None:
                                nn.init.constant_(m.bias.data, 0)
            
            def generate_samples(self, num_samples):
                self.generator.eval()
                with torch.no_grad():
                    z = torch.randn(num_samples, self.config.latent_dim, device=self.device)
                    samples = self.generator(z)
                return samples
            
            def get_model_info(self):
                algorithm = getattr(self.config, 'training_algorithm', 'backprop')
                discriminator_type = 'standard'
                if algorithm == 'forward_forward':
                    discriminator_type = 'forward_forward'
                elif algorithm == 'cafo':
                    discriminator_type = 'cafo'
                
                return {
                    'model_type': 'vanilla_gan',
                    'input_dim': self.config.input_dim,
                    'latent_dim': self.config.latent_dim,
                    'generator_layers': self.config.generator_layers,
                    'discriminator_layers': self.config.discriminator_layers,
                    'training_algorithm': algorithm,
                    'discriminator_type': discriminator_type,
                    'use_forward_forward': getattr(self.config, 'use_forward_forward', False),
                    'use_cafo': getattr(self.config, 'use_cafo', False),
                    'device': str(self.device),
                    'generator_params': sum(p.numel() for p in self.generator.parameters()),
                    'discriminator_params': sum(p.numel() for p in self.discriminator.parameters())
                }
        
        model = None
        
        # Get algorithm configuration
        algorithm = model_config.get('training_algorithm', 'backprop')
        use_forward_forward = model_config.get('use_forward_forward', False)
        use_cafo = model_config.get('use_cafo', False)
        
        # Handle boolean overrides
        if use_forward_forward:
            algorithm = 'forward_forward'
        elif use_cafo:
            algorithm = 'cafo'
        
        print(f"Building {model_type.upper()} with {algorithm.upper()} algorithm")
        print(f"Forward-Forward: {use_forward_forward}, CAFO: {use_cafo}")
        
        # Build Vanilla GAN model
        try:
            print("Creating Vanilla GAN model...")
            
            # Extract configuration from master config
            vanilla_config_dict = {
                'input_dim': model_config.get('input_dim', 784),
                'latent_dim': model_config.get('latent_dim', 100),
                'generator_layers': model_config.get('generator_layers', [256, 512, 1024]),
                'discriminator_layers': model_config.get('discriminator_layers', [1024, 512, 256]),
                'lr': model_config.get('learning_rate', 0.0002),
                'batch_size': model_config.get('batch_size', 64),
                'epochs': model_config.get('epochs', 200),
                'device': 'cuda' if torch.cuda.is_available() else 'cpu',
                'training_algorithm': algorithm,
                'use_forward_forward': use_forward_forward,
                'use_cafo': use_cafo,
                'use_wgan': model_config.get('use_wgan', False),
                'ff_theta': model_config.get('ff_theta', 2.0),
                'ff_positive_margin': model_config.get('ff_positive_margin', 2.0),
                'ff_negative_margin': model_config.get('ff_negative_margin', 0.0),
                'ff_blocks': model_config.get('ff_blocks', 3),
                'ff_epochs_per_block': model_config.get('ff_epochs_per_block', 10),
                'cafo_blocks': model_config.get('cafo_blocks', 3),
                'epochs_per_block': model_config.get('epochs_per_block', 10),
                'block_lr': model_config.get('block_lr', 0.001),
                'discriminator_dropout': model_config.get('discriminator_dropout', 0.3)
            }
            
            print(f"Vanilla GAN config: {vanilla_config_dict}")
            vanilla_config = VanillaGANConfig(**vanilla_config_dict)
            model = VanillaGAN(vanilla_config)
            
            print("Vanilla GAN model created successfully")
                
        except Exception as e:
            print(f"Failed to create Vanilla GAN model: {e}")
            traceback.print_exc()
            sys.exit(1)
        
        if model is None:
            print("Model creation failed")
            sys.exit(1)
        
        # Test model
        print("Testing Vanilla GAN model...")
        try:
            # Test generator
            z = torch.randn(4, model.config.latent_dim, device=model.device)
            with torch.no_grad():
                samples = model.generator(z)
            print(f"Generator test passed. Output shape: {samples.shape}")
            
            # Test discriminator
            fake_data = torch.randn(4, model.config.input_dim, device=model.device)
            with torch.no_grad():
                pred = model.discriminator(fake_data)
            print(f"Discriminator test passed. Output shape: {pred.shape}")
            
        except Exception as e:
            print(f"Model test failed: {e}")
            traceback.print_exc()
        
        # Update master config with build information
        model_info = model.get_model_info()
        gan_config['model'].update({
            'model_built': True,
            'model_class': 'VanillaGAN',
            'architecture': 'fully_connected',
            'latent_dim': model.config.latent_dim,
            'input_dim': model.config.input_dim,
            'generator_layers': model.config.generator_layers,
            'discriminator_layers': model.config.discriminator_layers,
            'training_algorithm': algorithm,
            'discriminator_type': model_info['discriminator_type'],
            'use_forward_forward': use_forward_forward,
            'use_cafo': use_cafo,
            'generator_params': sum(p.numel() for p in model.generator.parameters()),
            'discriminator_params': sum(p.numel() for p in model.discriminator.parameters()),
            'built_timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ'),
            'device': str(model.device)
        })
        
        gan_config['metadata'].update({
            'built_at': time.strftime('%Y-%m-%dT%H:%M:%SZ'),
            'build_step': 'completed',
            'model_created': True,
            'model_type': 'vanilla_gan',
            'architecture_type': 'fully_connected'
        })
        
        # Create detailed model info
        model_info_data = {
            'model_type': 'vanilla_gan',
            'model_name': args.model_name,
            'build_status': 'success',
            'model_class': 'VanillaGAN',
            'architecture': 'fully_connected',
            'latent_dim': model.config.latent_dim,
            'input_dim': model.config.input_dim,
            'generator_layers': model.config.generator_layers,
            'discriminator_layers': model.config.discriminator_layers,
            'training_algorithm': algorithm,
            'discriminator_type': model_info['discriminator_type'],
            'algorithm_config': {
                'use_forward_forward': use_forward_forward,
                'use_cafo': use_cafo,
                'ff_theta': vanilla_config_dict.get('ff_theta', 2.0),
                'ff_positive_margin': vanilla_config_dict.get('ff_positive_margin', 2.0),
                'ff_negative_margin': vanilla_config_dict.get('ff_negative_margin', 0.0),
                'ff_blocks': vanilla_config_dict.get('ff_blocks', 3),
                'cafo_blocks': vanilla_config_dict.get('cafo_blocks', 3)
            },
            'total_parameters': {
                'generator': sum(p.numel() for p in model.generator.parameters()),
                'discriminator': sum(p.numel() for p in model.discriminator.parameters()),
                'total': sum(p.numel() for p in model.generator.parameters()) + sum(p.numel() for p in model.discriminator.parameters())
            },
            'config': vanilla_config_dict,
            'pytorch_version': torch.__version__,
            'cuda_available': torch.cuda.is_available(),
            'device': str(model.device),
            'build_timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ'),
            'data_format': 'flattened_vectors'
        }
        
        # Encode updated config to base64
        updated_config_json = json.dumps(gan_config, indent=2)
        updated_config_base64 = base64.b64encode(updated_config_json.encode('utf-8')).decode('utf-8')
        
        # Save outputs with better error handling
        print("Saving model outputs...")
        
        # Save model
        os.makedirs(os.path.dirname(args.model_out) or '.', exist_ok=True)
        try:
            # Create a simplified model wrapper for reliable pickling
            class VanillaGANWrapper:
                def __init__(self, model_obj):
                    self.model_type = 'vanilla_gan'
                    self.config = model_obj.config.__dict__
                    self.generator_state = model_obj.generator.state_dict()
                    self.discriminator_state = model_obj.discriminator.state_dict()
                    self.generator_arch = model_obj.generator
                    self.discriminator_arch = model_obj.discriminator
                    self.device = str(model_obj.device)
                    self.model_info = model_obj.get_model_info()
            
            wrapper = VanillaGANWrapper(model)
            with open(args.model_out, 'wb') as f:
                pickle.dump(wrapper, f, protocol=pickle.HIGHEST_PROTOCOL)
            print(f"Model saved successfully: {args.model_out}")
            
        except Exception as e:
            print(f"Failed to save full model, saving state dict: {e}")
            # Fallback: save only state dicts and config
            model_state = {
                'model_type': 'vanilla_gan',
                'config': vanilla_config_dict,
                'generator_state_dict': model.generator.state_dict(),
                'discriminator_state_dict': model.discriminator.state_dict(),
                'model_info': model.get_model_info()
            }
            with open(args.model_out, 'wb') as f:
                pickle.dump(model_state, f, protocol=pickle.HIGHEST_PROTOCOL)
            print(f"Model state saved: {args.model_out}")
        
        # Save updated config
        os.makedirs(os.path.dirname(args.gan_config_base64_updated) or '.', exist_ok=True)
        with open(args.gan_config_base64_updated, 'w') as f:
            f.write(updated_config_base64)
        
        # Save model info
        os.makedirs(os.path.dirname(args.model_info) or '.', exist_ok=True)
        with open(args.model_info, 'w') as f:
            json.dump(model_info_data, f, indent=2)
        
        print("="*60)
        print("Vanilla GAN Build completed successfully")
        print(f"Algorithm: {algorithm.upper()}")
        print(f"Discriminator Type: {model_info['discriminator_type']}")
        print(f"Forward-Forward: {use_forward_forward}")
        print(f"CAFO: {use_cafo}")
        print(f"Model saved: {args.model_out}")
        print(f"Config (base64) saved: {args.gan_config_base64_updated}")
        print(f"Model info saved: {args.model_info}")
        print(f"Generator parameters: {sum(p.numel() for p in model.generator.parameters()):,}")
        print(f"Discriminator parameters: {sum(p.numel() for p in model.discriminator.parameters()):,}")
        print(f"Device: {model.device}")
        print("="*60)

    args:
      - --gan_config_json
      - {inputValue: gan_config_json}
      - --model_name
      - {inputValue: model_name}
      - --model_out
      - {outputPath: model_out}
      - --gan_config_base64_updated
      - {outputPath: gan_config_base64_updated}
      - --model_info
      - {outputPath: model_info}
