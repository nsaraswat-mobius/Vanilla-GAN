name: Upload Data To CDN v1
description: Uploads complete dataset set for Vanilla GAN pipeline continuation
inputs:
  - name: train_loader
    type: Dataset
    description: "Preprocessed training data in DataLoader format"
  - name: test_loader
    type: Dataset
    description: "Preprocessed test data in DataLoader format"
  - name: processed_data
    type: Dataset
    description: "Full processed dataset without train/test split"
  - name: gan_config_base64
    type: String
    description: "Updated master GAN configuration"
  - name: bearer_token
    type: String
    description: "Bearer token for authentication"
  - name: domain
    type: String
    description: "API domain"
  - name: get_cdn
    type: String
    description: "CDN base URL"

outputs:
  - name: train_loader_url
    type: String
    description: "URL to Vanilla GAN train loader (flattened data)"
  - name: test_loader_url
    type: String
    description: "URL to Vanilla GAN test loader (flattened data)"
  - name: processed_data_url
    type: String
    description: "URL to full processed Vanilla GAN dataset"
  - name: gan_config_base64_url
    type: String
    description: "URL to base64 encoded GAN config"
  - name: upload_summary
    type: String
    description: "Local upload summary"
  - name: upload_summary_url
    type: String
    description: "URL to upload summary"

implementation:
  container:
    image: kushagra4761/nesy-factory-gpu:t2.6v2
    command:
      - sh
      - -ec
      - |
        apt-get update > /dev/null && apt-get install -y curl > /dev/null
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, subprocess, json, os, uuid, pickle, sys, base64
        
        # ============================================================================
        # MAIN EXECUTION CODE
        # ============================================================================
        parser = argparse.ArgumentParser()
        
        # Processed data from Vanilla GAN Preprocess
        parser.add_argument('--train_loader', type=str, required=True)
        parser.add_argument('--test_loader', type=str, required=True)
        parser.add_argument('--processed_data', type=str, required=True)
        parser.add_argument('--gan_config_base64', type=str, required=True)
        
        # CDN credentials
        parser.add_argument('--bearer_token', type=str, required=True)
        parser.add_argument('--domain', type=str, required=True)
        parser.add_argument('--get_cdn', type=str, required=True)
        
        # Output URLs
        parser.add_argument('--train_loader_url', type=str, required=True)
        parser.add_argument('--test_loader_url', type=str, required=True)
        parser.add_argument('--processed_data_url', type=str, required=True)
        parser.add_argument('--gan_config_base64_url', type=str, required=True)
        parser.add_argument('--upload_summary', type=str, required=True)
        parser.add_argument('--upload_summary_url', type=str, required=True)
        
        args = parser.parse_args()
        
        print("=" * 80)
        print("UPLOAD DATA TO CDN v14 - VANILLA GAN COMPLETE SET")
        print("=" * 80)
        print("Specialized for Vanilla GAN (flattened data format)")
        print("=" * 80)
        
        # ============================================================================
        # Create ALL output directories before doing anything
        # ============================================================================
        print("\\nCreating output directories...")
        
        output_paths = [
            args.train_loader_url,
            args.test_loader_url,
            args.processed_data_url,
            args.gan_config_base64_url,
            args.upload_summary,
            args.upload_summary_url
        ]
        
        for path in output_paths:
            if path:
                dir_path = os.path.dirname(path)
                if dir_path and not os.path.exists(dir_path):
                    os.makedirs(dir_path, exist_ok=True)
                    print(f"  Created: {dir_path}")
        
        print(" All output directories created")
        
        # ============================================================================
        # Read bearer token
        # ============================================================================
        with open(args.bearer_token, 'r') as f:
            bearer_token = f.read().strip()
        
        upload_url = f"{args.domain}/mobius-content-service/v1.0/content/upload?filePathAccess=private&filePath=%2Fvanilla_gan%2F"
        
        # ============================================================================
        # File verification - VANILLA GAN SPECIFIC
        # ============================================================================
        print("\\nVerifying Vanilla GAN input files...")
        
        def check_file(file_path, file_type, description):
            if not os.path.exists(file_path):
                print(f" Missing: {file_path}")
                return False, 0, None
            
            file_size = os.path.getsize(file_path)
            size_kb = file_size / 1024
            
            try:
                if file_type == 'pickle':
                    with open(file_path, 'rb') as f:
                        obj = pickle.load(f)
                    
                    # Handle Vanilla GAN wrapper objects
                    if isinstance(obj, dict):
                        if 'loader' in obj:
                            # This is a train/test loader wrapper
                            samples = obj.get('num_samples', 'unknown')
                            model_type = obj.get('model_type', 'vanilla_gan')
                            input_dim = obj.get('input_dim', 'unknown')
                            split = obj.get('metadata', {}).get('split', 'unknown')
                            print(f" {description}: VanillaGAN {split} loader ({samples} samples, dim={input_dim}, {size_kb:.1f} KB)")
                            metadata = {
                                'type': 'vanilla_gan_loader',
                                'samples': samples,
                                'input_dim': input_dim,
                                'split': split,
                                'model_type': model_type
                            }
                        elif 'dataset' in obj:
                            # This is the full processed data wrapper
                            samples = obj.get('num_samples', 'unknown')
                            input_dim = obj.get('input_dim', 'unknown')
                            image_size = obj.get('image_size', 'unknown')
                            print(f" {description}: VanillaGAN full dataset ({samples} samples, dim={input_dim}, size={image_size}, {size_kb:.1f} KB)")
                            metadata = {
                                'type': 'vanilla_gan_full_dataset',
                                'samples': samples,
                                'input_dim': input_dim,
                                'image_size': image_size
                            }
                        else:
                            print(f" {description}: Dictionary ({len(obj)} keys, {size_kb:.1f} KB)")
                            metadata = {'type': 'dictionary'}
                    else:
                        print(f" {description}: {obj.__class__.__name__} ({size_kb:.1f} KB)")
                        metadata = {'type': 'object'}
                    
                    return True, size_kb, metadata
                    
                elif file_type == 'base64':
                    # Check if it's valid base64 and decode GAN config
                    with open(file_path, 'r') as f:
                        content = f.read()
                    
                    try:
                        decoded = base64.b64decode(content)
                        config = json.loads(decoded.decode('utf-8'))
                        model_type = config.get('model', {}).get('model_type', 'vanilla_gan')
                        input_dim = config.get('model', {}).get('input_dim', 'unknown')
                        train_samples = config.get('dataset', {}).get('train_samples', 0)
                        test_samples = config.get('dataset', {}).get('test_samples', 0)
                        
                        print(f" {description}: GAN Config ({model_type}, dim={input_dim}, train={train_samples}, test={test_samples}, {size_kb:.1f} KB)")
                        
                        metadata = {
                            'type': 'gan_config',
                            'model_type': model_type,
                            'input_dim': input_dim,
                            'train_samples': train_samples,
                            'test_samples': test_samples,
                            'config': config
                        }
                    except Exception as e:
                        print(f" {description}: Base64 file ({size_kb:.1f} KB) - Decode error: {str(e)[:50]}")
                        metadata = {'type': 'base64_file'}
                    
                    return True, size_kb, metadata
                    
                else:
                    print(f" {description}: ({size_kb:.1f} KB)")
                    return True, size_kb, {'type': 'unknown'}
                    
            except Exception as e:
                print(f" Error in {file_path}: {str(e)[:100]}")
                return False, size_kb, None
        
        # Check Vanilla GAN files
        files_to_check = [
            (args.train_loader, 'pickle', 'Train loader'),
            (args.test_loader, 'pickle', 'Test loader'),
            (args.processed_data, 'pickle', 'Full processed data'),
            (args.gan_config_base64, 'base64', 'GAN config base64')
        ]
        
        all_valid = True
        total_size = 0
        file_metadata = {}
        
        for file_path, file_type, description in files_to_check:
            valid, size_kb, metadata = check_file(file_path, file_type, description)
            if not valid:
                all_valid = False
            total_size += size_kb
            file_metadata[os.path.basename(file_path)] = metadata
        
        if not all_valid:
            print("\\n Some Vanilla GAN files are invalid!")
            sys.exit(1)
        
        print(f"\\n All Vanilla GAN files verified")
        print(f"  Total size: {total_size:.1f} KB")
        
        # ============================================================================
        # Upload function
        # ============================================================================
        def upload_file_to_cdn(file_path, output_url_path, description, file_tag):
            file_size = os.path.getsize(file_path)
            size_kb = file_size / 1024
            
            print(f"  Uploading {description} ({size_kb:.1f} KB)...")
            
            # Generate unique filename
            unique_id = str(uuid.uuid4())[:8]
            timestamp = uuid.uuid4().hex[:6]
            
            # Determine extension based on file type and tag
            if file_tag == 'gan_config_base64':
                file_ext = '.b64'
            elif 'loader' in file_tag or 'data' in file_tag:
                file_ext = '.pkl'
            else:
                file_ext = '.dat'
            
            cdn_filename = f"vanilla_gan_{file_tag}_{timestamp}_{unique_id}{file_ext}"
            
            curl_command = [
                "curl",
                "--location", upload_url,
                "--header", f"Authorization: Bearer {bearer_token}",
                "--form", f"file=@{file_path}",
                "--form", f"filename={cdn_filename}",
                "--fail",
                "--show-error",
                "--connect-timeout", "30",
                "--max-time", "120"
            ]
            
            try:
                process = subprocess.run(
                    curl_command,
                    capture_output=True,
                    text=True,
                    check=True
                )
                
                response_json = json.loads(process.stdout)
                relative_cdn_url = response_json.get("cdnUrl", "")
                
                if not relative_cdn_url:
                    print(f"     Error: No cdnUrl in response")
                    return None
                
                full_url = f"{args.get_cdn}{relative_cdn_url}"
                print(f"     Uploaded: {full_url[:60]}...")
                
                # Save URL locally
                with open(output_url_path, "w") as f:
                    f.write(full_url)
                
                return {
                    'url': full_url,
                    'size_bytes': file_size,
                    'size_kb': size_kb,
                    'description': description,
                    'file_tag': file_tag,
                    'cdn_filename': cdn_filename
                }
                
            except subprocess.CalledProcessError as e:
                print(f"     Upload failed: {e.stderr[:100]}")
                return None
            except Exception as e:
                print(f"    Error: {str(e)[:100]}")
                return None
        
        # ============================================================================
        # Upload VANILLA GAN files
        # ============================================================================
        print("\\n" + "=" * 80)
        print("UPLOADING VANILLA GAN DATASET SET")
        print("=" * 80)
        
        upload_results = {}
        files_to_upload = [
            (args.train_loader, args.train_loader_url, "Vanilla GAN train loader", "train_loader"),
            (args.test_loader, args.test_loader_url, "Vanilla GAN test loader", "test_loader"),
            (args.processed_data, args.processed_data_url, "Vanilla GAN full data", "processed_data"),
            (args.gan_config_base64, args.gan_config_base64_url, "GAN config base64", "gan_config_base64")
        ]
        
        # Upload each file
        for file_path, url_path, description, tag in files_to_upload:
            result = upload_file_to_cdn(file_path, url_path, description, tag)
            if result:
                upload_results[tag] = result
            else:
                print(f" Failed to upload: {tag}")
        
        # ============================================================================
        # Extract GAN config metadata
        # ============================================================================
        gan_config_metadata = file_metadata.get(os.path.basename(args.gan_config_base64), {})
        gan_config = gan_config_metadata.get('config', {})
        
        # ============================================================================
        # Create and upload summary
        # ============================================================================
        print("\\nCreating upload summary...")
        
        successful = len(upload_results)
        total_files = len(files_to_upload)
        total_bytes = sum(r['size_bytes'] for r in upload_results.values() if r)
        
        # Extract Vanilla GAN specific info
        model_type = gan_config.get('model', {}).get('model_type', 'vanilla_gan')
        input_dim = gan_config.get('model', {}).get('input_dim', 'unknown')
        image_size = gan_config.get('dataset', {}).get('actual_image_size', 'unknown')
        train_samples = gan_config.get('dataset', {}).get('train_samples', 0)
        test_samples = gan_config.get('dataset', {}).get('test_samples', 0)
        
        summary = {
            'pipeline_stage': 'vanilla_gan_data_upload',
            'gan_type': model_type,
            'upload_complete': successful == total_files,
            'total_files': total_files,
            'successful_uploads': successful,
            'total_size_bytes': total_bytes,
            'total_size_kb': total_bytes / 1024,
            'vanilla_gan_specific': {
                'input_dimension': input_dim,
                'image_size': image_size,
                'train_samples': train_samples,
                'test_samples': test_samples,
                'data_format': 'flattened_vectors',
                'is_flattened': True,
                'data_range': [-1, 1]
            },
            'files': {},
            'urls': {},
            'notes': 'Vanilla GAN dataset set uploaded for training pipeline continuation',
            'critical_files': [
                'train_loader',  # For training (flattened data)
                'test_loader',   # For evaluation (flattened data)
                'gan_config_base64'  # For model configuration
            ],
            'timestamp': uuid.uuid4().hex[:16]
        }
        
        # Add file details
        for tag, result in upload_results.items():
            if result:
                summary['files'][tag] = {
                    'description': result['description'],
                    'size_kb': result['size_kb'],
                    'uploaded': True,
                    'cdn_filename': result.get('cdn_filename', '')
                }
                summary['urls'][tag] = result['url']
        
        # Save summary locally
        with open(args.upload_summary, 'w') as f:
            json.dump(summary, f, indent=2)
        
        summary_size = os.path.getsize(args.upload_summary)
        print(f" Summary saved: {args.upload_summary} ({summary_size/1024:.1f} KB)")
        
        # Upload summary itself
        print("\\nUploading summary...")
        summary_result = upload_file_to_cdn(
            args.upload_summary, args.upload_summary_url,
            "Upload summary", "upload_summary"
        )
        
        if summary_result:
            upload_results['upload_summary'] = summary_result
            summary['urls']['upload_summary'] = summary_result['url']
            
            # Update local summary with its own URL
            with open(args.upload_summary, 'w') as f:
                json.dump(summary, f, indent=2)
        
        # ============================================================================
        # Final verification
        # ============================================================================
        print("\\n" + "=" * 80)
        print("FINAL VERIFICATION")
        print("=" * 80)
        
        # Check critical files for Vanilla GAN
        critical_files = ['train_loader', 'test_loader', 'gan_config_base64']
        missing_critical = [f for f in critical_files if f not in upload_results]
        
        if missing_critical:
            print(" MISSING CRITICAL FILES FOR VANILLA GAN:")
            for f in missing_critical:
                print(f"  - {f}")
            print("\\n  Vanilla GAN training pipeline may fail!")
        else:
            print(" All critical Vanilla GAN files uploaded successfully")
        
        # Verify URLs are saved
        url_files = [
            (args.train_loader_url, 'train_loader_url'),
            (args.test_loader_url, 'test_loader_url'),
            (args.processed_data_url, 'processed_data_url'),
            (args.gan_config_base64_url, 'gan_config_base64_url'),
            (args.upload_summary_url, 'upload_summary_url')
        ]
        
        url_count = 0
        for url_path, name in url_files:
            if os.path.exists(url_path):
                with open(url_path, 'r') as f:
                    url = f.read().strip()
                if url and url.startswith('http'):
                    url_count += 1
                    print(f"   {name}: URL saved")
                else:
                    print(f"   {name}: Invalid URL")
            else:
                print(f" {name}: File not created")
        
        print(f"\\nURLs saved: {url_count}/{len(url_files)}")
        
        # ============================================================================
        # Vanilla GAN specific instructions
        # ============================================================================
        print("\\n" + "=" * 80)
        print("VANILLA GAN PIPELINE REQUIREMENTS")
        print("=" * 80)
        print("These URLs will be sent to the Vanilla GAN training pipeline:")
        print("\\n1. FOR TRAINING (CRITICAL):")
        print("    train_loader_url: Flattened training data loader")
        print(f"     - Input dimension: {input_dim}")
        print(f"     - Training samples: {train_samples}")
        print("\\n2. FOR EVALUATION (CRITICAL):")
        print("    test_loader_url: Flattened test data loader")
        print(f"   Test samples: {test_samples}")
        print("    gan_config_base64_url: Model configuration")
        print("\\n3. FOR REFERENCE:")
        print("    processed_data_url: Full flattened dataset")
        print("    upload_summary_url: This summary")
        
        print("\\nIMPORTANT VANILLA GAN NOTES:")
        print(" Data is FLATTENED (1D vectors) not images")
        print(" Input dimension: {input_dim} (flattened from {image_size}x{image_size})")
        print(" Data range: [-1, 1] after normalization")
        
        if successful == total_files and summary_result:
            print("\\n" + "=" * 80)
            print(" VANILLA GAN DATASET SET UPLOADED SUCCESSFULLY!")
            print("=" * 80)
            print(f"All {successful} Vanilla GAN files + summary uploaded successfully!")
        else:
            print(f"\\n Upload incomplete: {successful}/{total_files} Vanilla GAN files")
            print("  Summary uploaded: " + ("Yes" if summary_result else "No"))
            sys.exit(1)
        
    args:
      # Vanilla GAN processed data inputs
      - --train_loader
      - {inputPath: train_loader}
      - --test_loader
      - {inputPath: test_loader}
      - --processed_data
      - {inputPath: processed_data}
      - --gan_config_base64
      - {inputPath: gan_config_base64}
      - --bearer_token
      - {inputPath: bearer_token}
      - --domain
      - {inputValue: domain}
      - --get_cdn
      - {inputValue: get_cdn}
      - --train_loader_url
      - {outputPath: train_loader_url}
      - --test_loader_url
      - {outputPath: test_loader_url}
      - --processed_data_url
      - {outputPath: processed_data_url}
      - --gan_config_base64_url
      - {outputPath: gan_config_base64_url}
      - --upload_summary
      - {outputPath: upload_summary}
      - --upload_summary_url
      - {outputPath: upload_summary_url}
