name: Download Vanilla GAN Data From CDN v1
description: Downloads Vanilla GAN dataset set from CDN for training pipeline
inputs:
  - name: train_loader_url
    type: String
    description: "URL to Vanilla GAN train loader (flattened data)"
  - name: test_loader_url
    type: String
    description: "URL to Vanilla GAN test loader (flattened data)"
  - name: processed_data_url
    type: String
    description: "URL to full processed Vanilla GAN dataset"
  - name: gan_config_base64_url
    type: String
    description: "URL to base64 encoded GAN config"
  - name: upload_summary_url
    type: String
    description: "URL to upload summary"
  - name: bearer_token
    type: String
    description: "Bearer token for authentication"

outputs:
  - name: train_loader
    type: Dataset
    description: "Vanilla GAN train loader (flattened data)"
  - name: test_loader
    type: Dataset
    description: "Vanilla GAN test loader (flattened data)"
  - name: processed_data
    type: Dataset
    description: "Full processed dataset without split (flattened)"
  - name: gan_config_base64
    type: String
    description: "Base64 encoded master GAN configuration"
  - name: download_summary
    type: String
    description: "Download summary"

implementation:
  container:
    image: kushagra4761/nesy-factory-gpu:t2.6v2
    command:
      - sh
      - -ec
      - |
        apt-get update > /dev/null && apt-get install -y curl > /dev/null
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse, subprocess, json, os, pickle, sys, base64, requests, urllib.parse
        
        # ============================================================================
        # MAIN EXECUTION CODE
        # ============================================================================
        parser = argparse.ArgumentParser()
        
        # Input URLs for Vanilla GAN
        parser.add_argument('--train_loader_url', type=str, required=True)
        parser.add_argument('--test_loader_url', type=str, required=True)
        parser.add_argument('--processed_data_url', type=str, required=True)
        parser.add_argument('--gan_config_base64_url', type=str, required=True)
        parser.add_argument('--upload_summary_url', type=str, required=True)
        
        # CDN credentials
        parser.add_argument('--bearer_token', type=str, required=True)
        
        # Output paths
        parser.add_argument('--train_loader', type=str, required=True)
        parser.add_argument('--test_loader', type=str, required=True)
        parser.add_argument('--processed_data', type=str, required=True)
        parser.add_argument('--gan_config_base64', type=str, required=True)
        parser.add_argument('--download_summary', type=str, required=True)
        
        args = parser.parse_args()
        
        print("=" * 80)
        print("DOWNLOAD VANILLA GAN DATA FROM CDN v1")
        print("=" * 80)
        print("Specialized for Vanilla GAN (flattened data format)")
        print("=" * 80)
        
        # ============================================================================
        # Create ALL output directories FIRST
        # ============================================================================
        print("\\nCreating output directories...")
        output_paths = [
            args.train_loader, args.test_loader, args.processed_data,
            args.gan_config_base64, args.download_summary
        ]
        
        for path in output_paths:
            if path:
                dir_path = os.path.dirname(path)
                if dir_path and not os.path.exists(dir_path):
                    os.makedirs(dir_path, exist_ok=True)
                    print(f"  Created: {dir_path}")
        
        print(" All output directories created")
        
        # Read bearer token
        with open(args.bearer_token, 'r') as f:
            bearer_token = f.read().strip()
        
        # ============================================================================
        # URL Decoding function (from your scripts)
        # ============================================================================
        def normalize_url(url: str) -> str:
            print(f"  Original URL: {url[:80]}...")
            
            # First, remove any spaces
            url = url.strip()
            
            # Decode URL-encoded characters
            url = urllib.parse.unquote(url)
            
            # Replace the specific patterns
            url = url.replace("_$_", "_$$")
            url = url.replace("_-", "_$$")
            url = url.replace("__DOLLARS__", "$$")
            
            # Decode common URL encodings
            replacements = {
                '%28': '(',    '%29': ')',    '%24': '$',
                '%2B': '+',    '%2F': '/',    '%3D': '=',
                '%3F': '?',    '%26': '&',    '%20': ' ',
                '%25': '%'
            }
            
            for encoded, decoded in replacements.items():
                url = url.replace(encoded, decoded)
            
            print(f"  Decoded URL: {url[:80]}...")
            return url
        
        # ============================================================================
        # Download function with Vanilla GAN specific verification
        # ============================================================================
        def download_from_cdn(url, output_path, description, file_tag):
            if not url or url.strip() == "":
                print(f"    Skipping {description}: Empty URL")
                return {'success': False, 'error': 'Empty URL'}
            
            print(f"   Downloading {description}...")
            
            # Normalize URL first
            url = normalize_url(url)
            
            curl_command = [
                "curl",
                "--location", url,
                "--header", f"Authorization: Bearer {bearer_token}",
                "--output", output_path,
                "--fail",
                "--show-error",
                "--connect-timeout", "30",
                "--max-time", "120"
            ]
            
            try:
                process = subprocess.run(
                    curl_command,
                    capture_output=True,
                    text=True,
                    check=True
                )
                
                if not os.path.exists(output_path):
                    return {'success': False, 'error': 'File not created'}
                
                file_size = os.path.getsize(output_path)
                size_kb = file_size / 1024
                
                print(f"    Downloaded: {file_size:,} bytes ({size_kb:.1f} KB)")
                
                # Verify the file based on type
                if file_tag in ['train_loader', 'test_loader', 'processed_data']:
                    try:
                        with open(output_path, 'rb') as f:
                            obj = pickle.load(f)
                        
                        # Handle Vanilla GAN wrapper
                        if isinstance(obj, dict):
                            if 'loader' in obj:
                                # DataLoader wrapper
                                samples = obj.get('num_samples', 'unknown')
                                model_type = obj.get('model_type', 'vanilla_gan')
                                input_dim = obj.get('input_dim', 'unknown')
                                print(f"     Vanilla GAN {description}")
                                print(f"       Model: {model_type}")
                                print(f"       Input dim: {input_dim}")
                                print(f"       Samples: {samples}")
                                
                                return {
                                    'success': True,
                                    'size_kb': size_kb,
                                    'type': 'vanilla_gan_loader',
                                    'model_type': model_type,
                                    'input_dim': input_dim,
                                    'samples': samples
                                }
                            else:
                                print(f"    Dictionary with {len(obj)} keys")
                                return {'success': True, 'size_kb': size_kb, 'type': 'dictionary'}
                        else:
                            class_name = obj.__class__.__name__
                            print(f"     {class_name} object")
                            return {'success': True, 'size_kb': size_kb, 'type': class_name}
                            
                    except Exception as e:
                        print(f"     Invalid pickle: {str(e)[:100]}")
                        return {'success': False, 'error': f'Invalid pickle: {str(e)[:50]}'}
                
                elif file_tag == 'gan_config_base64':
                    try:
                        with open(output_path, 'r') as f:
                            content = f.read().strip()
                        
                        # Verify it's valid base64
                        decoded = base64.b64decode(content)
                        config_json = decoded.decode('utf-8')
                        config = json.loads(config_json)
                        
                        model_type = config.get('model', {}).get('model_type', 'vanilla_gan')
                        input_dim = config.get('model', {}).get('input_dim', 'unknown')
                        
                        print(f"     Valid base64 GAN config")
                        print(f"       Model: {model_type}")
                        print(f"       Input dim: {input_dim}")
                        
                        return {
                            'success': True,
                            'size_kb': size_kb,
                            'type': 'base64_gan_config',
                            'model_type': model_type,
                            'input_dim': input_dim
                        }
                    except Exception as e:
                        print(f"     Invalid base64 or JSON: {str(e)[:100]}")
                        return {'success': False, 'error': f'Invalid base64: {str(e)[:50]}'}
                
                elif file_tag == 'upload_summary':
                    try:
                        with open(output_path, 'r') as f:
                            data = json.load(f)
                        print(f"     Valid JSON summary")
                        return {'success': True, 'size_kb': size_kb, 'type': 'json'}
                    except Exception as e:
                        print(f"     Invalid JSON: {str(e)[:100]}")
                        return {'success': False, 'error': f'Invalid JSON: {str(e)[:50]}'}
                
                else:
                    return {'success': True, 'size_kb': size_kb}
                    
            except subprocess.CalledProcessError as e:
                error_msg = e.stderr[:100] if e.stderr else str(e)
                print(f"     Download failed: {error_msg}")
                return {'success': False, 'error': error_msg}
            except Exception as e:
                print(f"     Error: {str(e)[:100]}")
                return {'success': False, 'error': str(e)[:100]}
        
        # ============================================================================
        # Download ALL Vanilla GAN files
        # ============================================================================
        print("\\n" + "=" * 80)
        print("DOWNLOADING VANILLA GAN DATASET SET")
        print("=" * 80)
        
        download_results = {}
        files_to_download = [
            # Vanilla GAN data loaders
            (args.train_loader_url, args.train_loader, "Vanilla GAN train loader", "train_loader"),
            (args.test_loader_url, args.test_loader, "Vanilla GAN test loader", "test_loader"),
            (args.processed_data_url, args.processed_data, "Vanilla GAN full data", "processed_data"),
            (args.gan_config_base64_url, args.gan_config_base64, "GAN config base64", "gan_config_base64"),
            (args.upload_summary_url, "/tmp/upload_summary.json", "Upload summary", "upload_summary")
        ]
        
        for url, output_path, description, tag in files_to_download:
            if tag == "upload_summary":
                # Download to temp location
                result = download_from_cdn(url, output_path, description, tag)
                download_results[tag] = result
            else:
                result = download_from_cdn(url, output_path, description, tag)
                download_results[tag] = result
        
        # ============================================================================
        # Extract GAN config metadata for verification
        # ============================================================================
        print("\\n" + "=" * 80)
        print("VANILLA GAN CONFIG VERIFICATION")
        print("=" * 80)
        
        gan_config_data = None
        gan_config_metadata = {}
        
        try:
            with open(args.gan_config_base64, 'r') as f:
                config_base64 = f.read().strip()
            
            decoded = base64.b64decode(config_base64)
            gan_config_data = json.loads(decoded.decode('utf-8'))
            
            model_config = gan_config_data.get('model', {})
            dataset_config = gan_config_data.get('dataset', {})
            
            gan_config_metadata = {
                'model_type': model_config.get('model_type', 'vanilla_gan'),
                'input_dim': model_config.get('input_dim', 784),
                'latent_dim': model_config.get('latent_dim', 100),
                'image_size': dataset_config.get('image_size', 28),
                'channels': dataset_config.get('channels', 1),
                'data_format': model_config.get('data_format', 'flattened')
            }
            
            print(f" GAN Config verified:")
            print(f"  Model Type: {gan_config_metadata['model_type']}")
            print(f"  Input Dimension: {gan_config_metadata['input_dim']}")
            print(f"  Latent Dimension: {gan_config_metadata['latent_dim']}")
            print(f"  Image Size: {gan_config_metadata['image_size']}")
            print(f"  Channels: {gan_config_metadata['channels']}")
            print(f"  Data Format: {gan_config_metadata['data_format']}")
            
        except Exception as e:
            print(f" Error reading GAN config: {str(e)[:100]}")
            gan_config_metadata = {'error': str(e)}
        
        # ============================================================================
        # Verify data loaders compatibility with config
        # ============================================================================
        print("\\n" + "=" * 80)
        print("DATA COMPATIBILITY VERIFICATION")
        print("=" * 80)
        
        compatibility_issues = []
        
        # Check train loader
        if 'train_loader' in download_results and download_results['train_loader'].get('success'):
            train_result = download_results['train_loader']
            loader_input_dim = train_result.get('input_dim', 'unknown')
            expected_input_dim = gan_config_metadata.get('input_dim', 784)
            
            if loader_input_dim != 'unknown' and expected_input_dim != 'unknown':
                if int(loader_input_dim) != int(expected_input_dim):
                    issue = f"Train loader input dim ({loader_input_dim}) doesn't match config ({expected_input_dim})"
                    compatibility_issues.append(issue)
                    print(f"  {issue}")
                else:
                    print(f" Train loader matches config (dim={loader_input_dim})")
        
        # Check processed data
        try:
            with open(args.processed_data, 'rb') as f:
                processed_obj = pickle.load(f)
            
            if isinstance(processed_obj, dict):
                proc_input_dim = processed_obj.get('input_dim', 'unknown')
                expected_input_dim = gan_config_metadata.get('input_dim', 784)
                
                if proc_input_dim != 'unknown' and expected_input_dim != 'unknown':
                    if int(proc_input_dim) != int(expected_input_dim):
                        issue = f"Processed data input dim ({proc_input_dim}) doesn't match config ({expected_input_dim})"
                        compatibility_issues.append(issue)
                        print(f"  {issue}")
                    else:
                        print(f" Processed data matches config (dim={proc_input_dim})")
                        
        except Exception as e:
            print(f"  Could not verify processed data: {str(e)[:50]}")
        
        # ============================================================================
        # Create download summary
        # ============================================================================
        print("\\nCreating download summary...")
        
        successful = sum(1 for r in download_results.values() if r.get('success', False))
        total = len(download_results)
        
        # Check critical files
        critical_checks = {
            'train_loader': download_results.get('train_loader', {}).get('success', False),
            'gan_config_base64': download_results.get('gan_config_base64', {}).get('success', False)
        }
        
        all_critical_present = all(critical_checks.values())
        
        summary = {
            'pipeline_stage': 'vanilla_gan_data_download',
            'download_complete': successful == total,
            'total_files': total,
            'successful_downloads': successful,
            'critical_files_present': all_critical_present,
            'critical_files': critical_checks,
            'gan_config': gan_config_metadata,
            'compatibility_issues': compatibility_issues,
            'file_details': {},
            'next_steps': []
        }
        
        # Add file details
        for key, result in download_results.items():
            if key != "upload_summary":  # Don't include temp file
                summary['file_details'][key] = {
                    'success': result.get('success', False),
                    'size_kb': result.get('size_kb', 0),
                    'type': result.get('type', 'N/A'),
                    'model_type': result.get('model_type', 'N/A'),
                    'input_dim': result.get('input_dim', 'N/A'),
                    'error': result.get('error', '')
                }
        
        # Add next steps
        if critical_checks['train_loader']:
            train_dim = download_results.get('train_loader', {}).get('input_dim', 'unknown')
            summary['next_steps'].append(f"Use train_loader for Vanilla GAN training (input_dim={train_dim})")
        
        if critical_checks['gan_config_base64']:
            summary['next_steps'].append("Use gan_config_base64 for model configuration")
        
        # Save summary
        with open(args.download_summary, 'w') as f:
            json.dump(summary, f, indent=2)
        
        summary_size = os.path.getsize(args.download_summary)
        print(f" Download summary saved: {args.download_summary} ({summary_size/1024:.1f} KB)")
        
        # ============================================================================
        # Load and test train loader
        # ============================================================================
        print("\\n" + "=" * 80)
        print("TESTING TRAIN LOADER")
        print("=" * 80)
        
        try:
            with open(args.train_loader, 'rb') as f:
                train_wrapper = pickle.load(f)
            
            if isinstance(train_wrapper, dict) and 'loader' in train_wrapper:
                loader = train_wrapper['loader']
                
                print(f" Train loader structure verified")
                print(f"  Number of batches: {len(loader)}")
                print(f"  Batch size: {loader.batch_size if hasattr(loader, 'batch_size') else 'unknown'}")
                
                # Test loading one batch
                try:
                    for batch in loader:
                        data = batch
                        if isinstance(batch, (list, tuple)):
                            data = batch[0]  # Usually (data, labels)
                        
                        print(f"  Batch shape: {data.shape}")
                        print(f"  Data type: {data.dtype}")
                        print(f"  Data range: [{data.min():.3f}, {data.max():.3f}]")
                        break  # Just first batch
                except Exception as e:
                    print(f"  Batch loading test failed: {str(e)[:100]}")
            else:
                print(f"  Train wrapper is not in expected format")
                
        except Exception as e:
            print(f" Error reading train loader: {str(e)[:100]}")
        
        # ============================================================================
        # Final status
        # ============================================================================
        print("\\n" + "=" * 80)
        print("DOWNLOAD COMPLETE - VANILLA GAN PIPELINE READINESS")
        print("=" * 80)
        
        print(f"Files downloaded: {successful}/{total}")
        print(f"Model Type: {gan_config_metadata.get('model_type', 'vanilla_gan')}")
        print(f"Input Dimension: {gan_config_metadata.get('input_dim', 'unknown')}")
        
        if all_critical_present:
            print(" ALL CRITICAL VANILLA GAN FILES DOWNLOADED SUCCESSFULLY")
            print("\\nReady for Vanilla GAN training:")
            print("  1. Train with: train_loader (flattened data)")
            print("  2. Configure with: gan_config_base64")
            print("  3. Evaluate with: test_loader")
            
            # Extract key parameters
            input_dim = gan_config_metadata.get('input_dim', 'unknown')
            latent_dim = gan_config_metadata.get('latent_dim', 'unknown')
            
            print(f"\\nKey parameters for training:")
            print(f"  Input dimension: {input_dim}")
            print(f"  Latent dimension: {latent_dim}")
            print(f"  Architecture: Fully connected (Vanilla GAN)")
            print(f"  Data format: Flattened vectors")
            
            print("\\n DOWNLOAD COMPLETE AND VERIFIED")
        else:
            print(" MISSING CRITICAL FILES FOR VANILLA GAN:")
            for file_name, present in critical_checks.items():
                if not present:
                    print(f"  - {file_name}")
            print("\\n  Vanilla GAN training pipeline may fail!")
            sys.exit(1)
        
    args:
      - --train_loader_url
      - {inputValue: train_loader_url}
      - --test_loader_url
      - {inputValue: test_loader_url}
      - --processed_data_url
      - {inputValue: processed_data_url}
      - --gan_config_base64_url
      - {inputValue: gan_config_base64_url}
      - --upload_summary_url
      - {inputValue: upload_summary_url}
      - --bearer_token
      - {inputPath: bearer_token}
      - --train_loader
      - {outputPath: train_loader}
      - --test_loader
      - {outputPath: test_loader}
      - --processed_data
      - {outputPath: processed_data}
      - --gan_config_base64
      - {outputPath: gan_config_base64}
      - --download_summary
      - {outputPath: download_summary}
