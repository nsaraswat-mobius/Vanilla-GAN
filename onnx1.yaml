name: Vanilla GAN ONNX Converter 2
description: Converts a PyTorch Vanilla GAN model (generator and discriminator) to ONNX format by reading configuration from a Triton config.pbtxt file.
inputs:
  - {name: model_file, type: Model, description: "Directory containing model.pt"}
  - {name: config_file, type: Model, description: "Directory containing config.pbtxt file"}
outputs:
  - {name: generator_onnx, type: Model, description: "Converted generator ONNX model"}
  - {name: generator_data, type: Model, description: "Generator ONNX data files (if separated)"}
  - {name: discriminator_onnx, type: Model, description: "Converted discriminator ONNX model"}
  - {name: discriminator_data, type: Model, description: "Discriminator ONNX data files (if separated)"}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v25
    command:
      - sh
      - -c
      - |
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet onnx onnxruntime || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet onnx onnxruntime --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        import re
        import sys
        import torch
        import torch.onnx
        from torch import nn
        import numpy as np
        import onnx
        import onnxruntime as ort
        import pickle
        import shutil
        import datetime
        from pathlib import Path

        # Define model classes
        class VanillaGenerator(nn.Module):
            def __init__(self, latent_dim, output_dim, hidden_layers=[256, 512, 1024]):
                super(VanillaGenerator, self).__init__()
                self.latent_dim = latent_dim
                
                layers = []
                input_dim = latent_dim
                
                for hidden_dim in hidden_layers:
                    layers.append(nn.Linear(input_dim, hidden_dim))
                    layers.append(nn.ReLU())
                    input_dim = hidden_dim
                
                layers.append(nn.Linear(input_dim, output_dim))
                layers.append(nn.Tanh())
                
                self.model = nn.Sequential(*layers)
            
            def forward(self, z):
                return self.model(z)

        class VanillaDiscriminator(nn.Module):
            def __init__(self, input_dim, hidden_layers=[1024, 512, 256]):
                super(VanillaDiscriminator, self).__init__()
                self.input_dim = input_dim
                
                layers = []
                current_dim = input_dim
                
                for hidden_dim in hidden_layers:
                    layers.append(nn.Linear(current_dim, hidden_dim))
                    layers.append(nn.LeakyReLU(0.2))
                    layers.append(nn.Dropout(0.3))
                    current_dim = hidden_dim
                
                layers.append(nn.Linear(current_dim, 1))
                layers.append(nn.Sigmoid())
                
                self.model = nn.Sequential(*layers)
            
            def forward(self, x):
                return self.model(x.view(x.size(0), -1))

        class BalancedVanillaGAN:
            def __init__(self):
                self.generator = None
                self.discriminator = None
                self.config = {}

        def export_to_onnx_with_external_data(model, dummy_input, output_path, input_names, output_names, dynamic_axes):
            # First export to a temporary location
            temp_path = output_path + ".tmp"
            
            torch.onnx.export(
                model,
                dummy_input,
                temp_path,
                export_params=True,
                opset_version=17,
                do_constant_folding=True,
                input_names=input_names,
                output_names=output_names,
                dynamic_axes=dynamic_axes
            )
            
            # Check file size
            file_size_mb = os.path.getsize(temp_path) / 1024 / 1024
            print(f"  Initial ONNX file size: {file_size_mb:.2f} MB")
            
            # If file is large (>2GB limit for protobuf), use external data
            # We'll use 100MB as threshold to be safe
            if file_size_mb > 100:
                print(f"  Model size exceeds 100MB, using external data format...")
                onnx_model = onnx.load(temp_path, load_external_data=False)
                
                # Save with external data
                onnx.save_model(
                    onnx_model,
                    output_path,
                    save_as_external_data=True,
                    all_tensors_to_one_file=True,
                    location="model.onnx.data",
                    size_threshold=1024,  # 1KB threshold
                    convert_attribute=False
                )
                
                # Remove temporary file
                os.remove(temp_path)
                
                # Check if data file was created
                data_file = output_path + ".data"
                if os.path.exists(data_file):
                    print(f"  External data file created: {os.path.basename(data_file)}")
                    print(f"  Data file size: {os.path.getsize(data_file) / 1024 / 1024:.2f} MB")
                    return data_file
                else:
                    print(f"  No separate data file created (weights embedded)")
                    return None
            else:
                # Model is small enough, just rename temp file
                os.rename(temp_path, output_path)
                print(f"  Model is small enough, weights embedded in ONNX file")
                return None

        parser = argparse.ArgumentParser()
        parser.add_argument('--model_file', type=str, required=True)
        parser.add_argument('--config_file', type=str, required=True)
        parser.add_argument('--generator_onnx', type=str, required=True)
        parser.add_argument('--generator_data', type=str, required=True)
        parser.add_argument('--discriminator_onnx', type=str, required=True)
        parser.add_argument('--discriminator_data', type=str, required=True)
        args = parser.parse_args()

        model_path = os.path.join(args.model_file, "model.pt")
        config_path = os.path.join(args.config_file, "config.pbtxt")

        if not os.path.exists(model_path):
            print(f"ERROR: model.pt not found at {model_path}")
            sys.exit(1)

        if not os.path.exists(config_path):
            print(f"ERROR: config.pbtxt not found at {config_path}")
            sys.exit(1)

        # =============================
        # Parse config.pbtxt for configuration
        # =============================
        print("Reading config.pbtxt to extract model configuration...")

        with open(config_path, "r") as f:
            content = f.read()

        # Extract key configuration parameters
        config_dict = {
            'latent_dim': 100,
            'input_dim': 784,
            'generator_layers': [256, 512, 1024],
            'discriminator_layers': [1024, 512, 256],
            'training_algorithm': 'backprop'
        }

        # Try to extract from config.pbtxt if available
        latent_match = re.search(r"latent_dim:\s*(\d+)", content)
        if latent_match:
            config_dict['latent_dim'] = int(latent_match.group(1))

        output_match = re.search(r"output_dim:\s*(\d+)", content)
        if output_match:
            config_dict['input_dim'] = int(output_match.group(1))

        input_match = re.search(r"input_dim:\s*(\d+)", content)
        if input_match:
            config_dict['input_dim'] = int(input_match.group(1))

        print(f"Parsed configuration: {config_dict}")

        # Create output directories
        os.makedirs(args.generator_onnx, exist_ok=True)
        os.makedirs(args.generator_data, exist_ok=True)
        os.makedirs(args.discriminator_onnx, exist_ok=True)
        os.makedirs(args.discriminator_data, exist_ok=True)

        # ============================
        # Load PyTorch model
        # ============================
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"Device: {device}")

        print("Loading model...")
        try:
            saved_data = torch.load(model_path, map_location=device, weights_only=False)
        except Exception as e:
            with open(model_path, 'rb') as f:
                saved_data = pickle.load(f)

        # Extract model components
        if hasattr(saved_data, 'generator') and hasattr(saved_data, 'discriminator'):
            print("Detected BalancedVanillaGAN model format")
            generator_state = saved_data.generator.state_dict()
            discriminator_state = saved_data.discriminator.state_dict()
            if hasattr(saved_data, 'config'):
                config_dict.update(saved_data.config)
        elif isinstance(saved_data, dict):
            generator_state = saved_data.get('generator_state_dict', saved_data.get('generator_state'))
            discriminator_state = saved_data.get('discriminator_state_dict', saved_data.get('discriminator_state'))
            if 'config' in saved_data:
                config_dict.update(saved_data['config'])

        # Create models
        print("Creating generator and discriminator...")
        generator = VanillaGenerator(
            config_dict['latent_dim'],
            config_dict['input_dim'],
            config_dict['generator_layers']
        ).to(device)

        discriminator = VanillaDiscriminator(
            config_dict['input_dim'],
            config_dict['discriminator_layers']
        ).to(device)

        # Load weights
        generator.load_state_dict(generator_state)
        discriminator.load_state_dict(discriminator_state)
        generator.eval()
        discriminator.eval()

        # ============================
        # Export Generator to ONNX
        # ============================
        print("\\n" + "="*60)
        print("Exporting Generator to ONNX...")
        print("="*60)

        z = torch.randn(1, config_dict['latent_dim']).to(device)
        generator_path = os.path.join(args.generator_onnx, "model.onnx")

        gen_data_file = export_to_onnx_with_external_data(
            generator,
            z,
            generator_path,
            input_names=['latent_vector'],
            output_names=['generated_samples'],
            dynamic_axes={
                'latent_vector': {0: 'batch_size'},
                'generated_samples': {0: 'batch_size'}
            }
        )

        print(f"Generator saved to: {generator_path}")

        # Handle generator data file
        if gen_data_file and os.path.exists(gen_data_file):
            data_dst = os.path.join(args.generator_data, os.path.basename(gen_data_file))
            shutil.copy(gen_data_file, data_dst)
            print(f"Generator data file copied to: {data_dst}")
        else:
            # Create a placeholder to satisfy Kubeflow output requirements
            placeholder_path = os.path.join(args.generator_data, "no_external_data.txt")
            with open(placeholder_path, 'w') as f:
                f.write("Model weights are embedded in the ONNX file\\n")
                f.write("No external .data file was created\\n")
            print(f"Created placeholder at: {placeholder_path}")

        # ============================
        # Export Discriminator to ONNX
        # ============================
        print("\\n" + "="*60)
        print("Exporting Discriminator to ONNX...")
        print("="*60)

        x = torch.randn(1, config_dict['input_dim']).to(device)
        discriminator_path = os.path.join(args.discriminator_onnx, "model.onnx")

        disc_data_file = export_to_onnx_with_external_data(
            discriminator,
            x,
            discriminator_path,
            input_names=['input_samples'],
            output_names=['realism_scores'],
            dynamic_axes={
                'input_samples': {0: 'batch_size'},
                'realism_scores': {0: 'batch_size'}
            }
        )

        print(f"Discriminator saved to: {discriminator_path}")

        # Handle discriminator data file
        if disc_data_file and os.path.exists(disc_data_file):
            data_dst = os.path.join(args.discriminator_data, os.path.basename(disc_data_file))
            shutil.copy(disc_data_file, data_dst)
            print(f"Discriminator data file copied to: {data_dst}")
        else:
            # Create a placeholder to satisfy Kubeflow output requirements
            placeholder_path = os.path.join(args.discriminator_data, "no_external_data.txt")
            with open(placeholder_path, 'w') as f:
                f.write("Model weights are embedded in the ONNX file\\n")
                f.write("No external .data file was created\\n")
            print(f"Created placeholder at: {placeholder_path}")

        # ============================
        # Validate ONNX models
        # ============================
        print("\\n" + "="*60)
        print("Validating ONNX models...")
        print("="*60)

        try:
            gen_model = onnx.load(generator_path)
            onnx.checker.check_model(gen_model)
            print("Generator model is valid")
        except Exception as e:
            print(f"Generator validation failed: {e}")
            sys.exit(1)

        try:
            disc_model = onnx.load(discriminator_path)
            onnx.checker.check_model(disc_model)
            print("Discriminator model is valid")
        except Exception as e:
            print(f"Discriminator validation failed: {e}")
            sys.exit(1)

        # ============================
        # Smoke test with ONNX Runtime
        # ============================
        print("\\n" + "="*60)
        print("Running ONNX Runtime smoke test...")
        print("="*60)

        try:
            gen_session = ort.InferenceSession(generator_path)
            disc_session = ort.InferenceSession(discriminator_path)

            z_test = torch.randn(1, config_dict['latent_dim']).cpu().numpy().astype(np.float32)
            generated = gen_session.run(None, {'latent_vector': z_test})[0]
            scores = disc_session.run(None, {'input_samples': generated.astype(np.float32)})[0]

            print(f"Generated samples shape: {generated.shape}")
            print(f"Realism scores shape: {scores.shape}")
            print(f"Score range: [{scores.min():.4f}, {scores.max():.4f}]")
        except Exception as e:
            print(f"ONNX Runtime test failed: {e}")
            sys.exit(1)

        # ============================
        # Create manifest files
        # ============================
        print("\\n" + "="*60)
        print("Creating manifest files...")
        print("="*60)

        def create_manifest_file(output_dir, model_name):
            manifest_path = os.path.join(output_dir, "MANIFEST")
            with open(manifest_path, 'w') as f:
                f.write(f"Artifacts for {model_name}\\n")
                f.write(f"Generated at: {datetime.datetime.now().isoformat()}\\n")
                f.write(f"\\nFiles:\\n")
                
                # List all files in directory
                for root, dirs, files in os.walk(output_dir):
                    for file in files:
                        if file == "MANIFEST":
                            continue
                        file_path = os.path.join(root, file)
                        rel_path = os.path.relpath(file_path, output_dir)
                        file_size = os.path.getsize(file_path)
                        f.write(f"  {rel_path}: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)\\n")
            print(f"  Created manifest: {manifest_path}")

        create_manifest_file(args.generator_onnx, "Generator ONNX")
        create_manifest_file(args.generator_data, "Generator Data")
        create_manifest_file(args.discriminator_onnx, "Discriminator ONNX")
        create_manifest_file(args.discriminator_data, "Discriminator Data")

        # ============================
        # Final Summary
        # ============================
        print("\\n" + "="*60)
        print("CONVERSION COMPLETED SUCCESSFULLY!")
        print("="*60)
        print(f"\\nOutput directories:")
        print(f"  Generator ONNX: {args.generator_onnx}/")
        print(f"  Generator Data: {args.generator_data}/")
        print(f"  Discriminator ONNX: {args.discriminator_onnx}/")
        print(f"  Discriminator Data: {args.discriminator_data}/")

        # List contents of each directory
        print("\\nContents:")
        for dir_name, dir_path in [
            ("Generator ONNX", args.generator_onnx),
            ("Generator Data", args.generator_data),
            ("Discriminator ONNX", args.discriminator_onnx),
            ("Discriminator Data", args.discriminator_data)
        ]:
            print(f"\\n  {dir_name}:")
            if os.path.exists(dir_path):
                for item in sorted(os.listdir(dir_path)):
                    item_path = os.path.join(dir_path, item)
                    if os.path.isfile(item_path):
                        size = os.path.getsize(item_path)
                        print(f"    - {item} ({size / 1024:.2f} KB)")
                    else:
                        print(f"    - {item}/ (directory)")
            else:
                print(f"    Directory does not exist!")

        print("\\n" + "="*60)
        print("Ready for Triton deployment!")
        print("="*60)

    args:
      - --model_file
      - {inputPath: model_file}
      - --config_file
      - {inputPath: config_file}
      - --generator_onnx
      - {outputPath: generator_onnx}
      - --generator_data
      - {outputPath: generator_data}
      - --discriminator_onnx
      - {outputPath: discriminator_onnx}
      - --discriminator_data
      - {outputPath: discriminator_data}
