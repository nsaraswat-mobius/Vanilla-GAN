name: Create Continual Learning Tasks for Vanilla GAN
description: Splits data into multiple tasks for Vanilla GAN continual learning in failure signature anomaly detection.
inputs:
  - {name: train_loader, type: Dataset}
  - {name: test_loader, type: Dataset}
  - {name: config, type: String}
outputs:
  - {name: tasks, type: Dataset}
implementation:
  container:
    image: nikhilv215/nesy-factory:v23
    command:
    - python3
    - -c
    - |
        import argparse
        import json
        import os
        import pickle
        import numpy as np
        import torch
        from torch.utils.data import TensorDataset, DataLoader
        
        class GANDataWrapper:
            def __init__(self, dataset, model_type='vanilla_gan', image_size=64, channels=3):
                self.dataset = dataset
                self.model_type = model_type
                self.image_size = image_size
                self.channels = channels
                self.num_samples = len(dataset)
            
            def __len__(self):
                return len(self.dataset)
            
            def __getitem__(self, idx):
                return self.dataset[idx]

        class GANTaskDataset:
            def __init__(self, images, labels, task_id=0, domain_factor=0.0, 
                         noise_level=0.0, blur_factor=0.0):
                self.images = images
                self.labels = labels
                self.task_id = task_id
                self.domain_factor = domain_factor
                self.noise_level = noise_level
                self.blur_factor = blur_factor
            
            def __len__(self):
                return len(self.images)
            
            def __getitem__(self, idx):
                img = self.images[idx]
                label = self.labels[idx]
                
                # Apply domain shift transformations for GAN
                if self.domain_factor > 0:
                    img = self.apply_domain_shift(img, self.domain_factor)
                
                return img, label
            
            def apply_domain_shift(self, img, factor):
                img = img.clone()
                
                # Add noise based on domain factor
                if self.noise_level > 0:
                    noise = torch.randn_like(img) * self.noise_level * factor
                    img = img + noise
                    img = torch.clamp(img, -1, 1)
                
                # Apply blur based on domain factor
                if self.blur_factor > 0 and img.dim() > 2:
                    from torch.nn.functional import avg_pool2d
                    kernel_size = int(3 + self.blur_factor * factor * 5)
                    if kernel_size % 2 == 0:
                        kernel_size += 1
                    
                    if img.dim() == 3:
                        img = avg_pool2d(img.unsqueeze(0), 
                                       kernel_size=kernel_size, 
                                       stride=1, 
                                       padding=kernel_size//2).squeeze(0)
                
                return img

        class GANTemporalDataSplitter:
          
            def __init__(self, data, config, strategy='temporal_split'):
                self.data = data
                self.config = config
                self.strategy = strategy
                
            def create_continual_tasks(self, num_tasks: int = 3) -> list:
                if self.strategy == 'temporal_split':
                    return self._temporal_split(num_tasks)
                elif self.strategy == 'domain_split':
                    return self._domain_split(num_tasks)
                elif self.strategy == 'complexity_split':
                    return self._complexity_split(num_tasks)
                elif self.strategy == 'class_split':
                    return self._class_split(num_tasks)
                else:
                    return self._temporal_split(num_tasks)
            
            def _temporal_split(self, num_tasks: int) -> list:
                tasks = []
                
                X_train = self.data['X_train']
                y_train = self.data['y_train']
                X_test = self.data['X_test']
                y_test = self.data['y_test']

                train_size = len(X_train)
                test_size = len(X_test)
                
                # Split chronologically
                train_splits = np.array_split(range(train_size), num_tasks)
                test_splits = np.array_split(range(test_size), num_tasks)
                
                for i in range(num_tasks):
                    # Get data for this task
                    train_idx = train_splits[i]
                    test_idx = test_splits[i]
                    
                    task_X_train = X_train[train_idx]
                    task_y_train = y_train[train_idx]
                    task_X_test = X_test[test_idx]
                    task_y_test = y_test[test_idx]
                    
                    # Simulate temporal evolution for GAN
                    temporal_factor = i / (num_tasks - 1) if num_tasks > 1 else 0
                    
                    # Create GAN-specific dataset with domain shift
                    train_dataset = GANTaskDataset(
                        images=task_X_train,
                        labels=task_y_train,
                        task_id=i,
                        domain_factor=temporal_factor,
                        noise_level=0.1,
                        blur_factor=0.1
                    )
                    
                    test_dataset = GANTaskDataset(
                        images=task_X_test,
                        labels=task_y_test,
                        task_id=i,
                        domain_factor=temporal_factor,
                        noise_level=0.1,
                        blur_factor=0.1
                    )

                    # Create data loaders
                    batch_size = self.config.get('batch_size', 32)
                    train_loader = DataLoader(
                        train_dataset, 
                        batch_size=batch_size, 
                        shuffle=True,
                        num_workers=self.config.get('num_workers', 0)
                    )
                    
                    test_loader = DataLoader(
                        test_dataset, 
                        batch_size=batch_size, 
                        shuffle=False,
                        num_workers=self.config.get('num_workers', 0)
                    )
                    
                    task_data = {
                        'task_id': i,
                        'X_train': task_X_train,
                        'y_train': task_y_train,
                        'X_test': task_X_test,
                        'y_test': task_y_test,
                        'train_dataset': train_dataset,
                        'test_dataset': test_dataset,
                        'train_loader': train_loader,
                        'test_loader': test_loader,
                        'temporal_factor': float(temporal_factor),
                        'description': f'Temporal Period {i+1}/{num_tasks}',
                        'split_type': 'temporal'
                    }
                    
                    # Add task-specific statistics
                    task_data['stats'] = {
                        'train_samples': len(task_X_train),
                        'test_samples': len(task_X_test),
                        'image_shape': task_X_train.shape[1:],
                        'temporal_factor': float(temporal_factor),
                        'train_mean': np.mean(task_X_train, axis=(0, 2, 3)).tolist() if task_X_train.ndim == 4 else np.mean(task_X_train, axis=0).tolist(),
                        'train_std': np.std(task_X_train, axis=(0, 2, 3)).tolist() if task_X_train.ndim == 4 else np.std(task_X_train, axis=0).tolist()
                    }
                    
                    tasks.append(task_data)
                
                return tasks
            
            def _domain_split(self, num_tasks: int) -> list:
                tasks = []
                
                X_train = self.data['X_train']
                y_train = self.data['y_train']
                X_test = self.data['X_test']
                y_test = self.data['y_test']
                
                train_size = len(X_train)
                test_size = len(X_test)
                
                train_splits = np.array_split(range(train_size), num_tasks)
                test_splits = np.array_split(range(test_size), num_tasks)
                
                # Different domain styles for GAN
                domain_styles = ['noise', 'blur', 'contrast', 'color_shift', 'original']
                
                for i in range(num_tasks):
                    train_idx = train_splits[i]
                    test_idx = test_splits[i]
                    
                    task_X_train = X_train[train_idx]
                    task_y_train = y_train[train_idx]
                    task_X_test = X_test[test_idx]
                    task_y_test = y_test[test_idx]
                    
                    # Assign domain style
                    domain_style = domain_styles[i % len(domain_styles)]
                    domain_factor = i * 0.25  # Increasing domain shift
                    
                    # Configure domain parameters based on style
                    noise_level = 0.1 if domain_style == 'noise' else 0.0
                    blur_factor = 0.1 if domain_style == 'blur' else 0.0
                    color_factor = 0.1 if domain_style == 'color_shift' else 0.0
                    
                    # Create domain-specific dataset
                    train_dataset = GANTaskDataset(
                        images=task_X_train,
                        labels=task_y_train,
                        task_id=i,
                        domain_factor=domain_factor,
                        noise_level=noise_level,
                        blur_factor=blur_factor
                    )
                    
                    test_dataset = GANTaskDataset(
                        images=task_X_test,
                        labels=task_y_test,
                        task_id=i,
                        domain_factor=domain_factor,
                        noise_level=noise_level,
                        blur_factor=blur_factor
                    )

                    batch_size = self.config.get('batch_size', 32)
                    train_loader = DataLoader(
                        train_dataset, 
                        batch_size=batch_size, 
                        shuffle=True,
                        num_workers=self.config.get('num_workers', 0)
                    )
                    
                    test_loader = DataLoader(
                        test_dataset, 
                        batch_size=batch_size, 
                        shuffle=False,
                        num_workers=self.config.get('num_workers', 0)
                    )
                    
                    task_data = {
                        'task_id': i,
                        'X_train': task_X_train,
                        'y_train': task_y_train,
                        'X_test': task_X_test,
                        'y_test': task_y_test,
                        'train_dataset': train_dataset,
                        'test_dataset': test_dataset,
                        'train_loader': train_loader,
                        'test_loader': test_loader,
                        'domain_style': domain_style,
                        'domain_factor': float(domain_factor),
                        'description': f'Domain {i+1}: {domain_style} (shift: {domain_factor:.2f})',
                        'split_type': 'domain'
                    }
                    
                    task_data['stats'] = {
                        'train_samples': len(task_X_train),
                        'test_samples': len(task_X_test),
                        'image_shape': task_X_train.shape[1:],
                        'domain_style': domain_style,
                        'domain_factor': float(domain_factor),
                        'train_mean': np.mean(task_X_train, axis=(0, 2, 3)).tolist() if task_X_train.ndim == 4 else np.mean(task_X_train, axis=0).tolist()
                    }
                    
                    tasks.append(task_data)
                
                return tasks
            
            def _complexity_split(self, num_tasks: int) -> list:
                tasks = []
                
                X_train = self.data['X_train']
                y_train = self.data['y_train']
                X_test = self.data['X_test']
                y_test = self.data['y_test']
                
                # Calculate complexity (variance) for images
                if X_train.ndim == 4:  # Image data (batch, channels, height, width)
                    complexity_scores = np.var(X_train, axis=(1, 2, 3))
                else:
                    complexity_scores = np.var(X_train, axis=1)
                
                train_sorted_indices = np.argsort(complexity_scores)
                test_indices = np.random.permutation(len(X_test))
                
                train_splits = np.array_split(train_sorted_indices, num_tasks)
                test_splits = np.array_split(test_indices, num_tasks)
                
                for i in range(num_tasks):
                    train_idx = train_splits[i]
                    test_idx = test_splits[i]
                    
                    task_X_train = X_train[train_idx]
                    task_y_train = y_train[train_idx]
                    task_X_test = X_test[test_idx]
                    task_y_test = y_test[test_idx]
                    
                    # Complexity-based domain factor
                    complexity_desc = "Easy" if i == 0 else "Hard" if i == num_tasks-1 else "Medium"
                    complexity_factor = i / (num_tasks - 1) if num_tasks > 1 else 0
                    
                    # Adjust transformations based on complexity
                    noise_level = 0.05 + complexity_factor * 0.2
                    blur_factor = 0.05 + complexity_factor * 0.15
                    
                    train_dataset = GANTaskDataset(
                        images=task_X_train,
                        labels=task_y_train,
                        task_id=i,
                        domain_factor=complexity_factor,
                        noise_level=noise_level,
                        blur_factor=blur_factor
                    )
                    
                    test_dataset = GANTaskDataset(
                        images=task_X_test,
                        labels=task_y_test,
                        task_id=i,
                        domain_factor=complexity_factor,
                        noise_level=noise_level,
                        blur_factor=blur_factor
                    )

                    batch_size = self.config.get('batch_size', 32)
                    train_loader = DataLoader(
                        train_dataset, 
                        batch_size=batch_size, 
                        shuffle=True,
                        num_workers=self.config.get('num_workers', 0)
                    )
                    
                    test_loader = DataLoader(
                        test_dataset, 
                        batch_size=batch_size, 
                        shuffle=False,
                        num_workers=self.config.get('num_workers', 0)
                    )
                    
                    task_data = {
                        'task_id': i,
                        'X_train': task_X_train,
                        'y_train': task_y_train,
                        'X_test': task_X_test,
                        'y_test': task_y_test,
                        'train_dataset': train_dataset,
                        'test_dataset': test_dataset,
                        'train_loader': train_loader,
                        'test_loader': test_loader,
                        'complexity_level': complexity_desc,
                        'complexity_factor': float(complexity_factor),
                        'description': f'Complexity Level {i+1}/{num_tasks} ({complexity_desc})',
                        'split_type': 'complexity'
                    }
                    
                    task_complexity = np.mean(complexity_scores[train_idx])
                    task_data['stats'] = {
                        'train_samples': len(task_X_train),
                        'test_samples': len(task_X_test),
                        'image_shape': task_X_train.shape[1:],
                        'complexity_level': complexity_desc,
                        'complexity_score': float(task_complexity),
                        'complexity_factor': float(complexity_factor),
                        'train_mean': np.mean(task_X_train, axis=(0, 2, 3)).tolist() if task_X_train.ndim == 4 else np.mean(task_X_train, axis=0).tolist()
                    }
                    
                    tasks.append(task_data)
                
                return tasks
            
            def _class_split(self, num_tasks: int) -> list:
                tasks = []
                
                X_train = self.data['X_train']
                y_train = self.data['y_train']
                X_test = self.data['X_test']
                y_test = self.data['y_test']
                
                train_size = len(X_train)
                test_size = len(X_test)
                
                train_splits = np.array_split(range(train_size), num_tasks)
                test_splits = np.array_split(range(test_size), num_tasks)
                
                # Simulated class types for GAN
                class_types = ['sharp', 'smooth', 'textured', 'simple', 'complex']
                
                for i in range(num_tasks):
                    train_idx = train_splits[i]
                    test_idx = test_splits[i]
                    
                    task_X_train = X_train[train_idx]
                    task_y_train = y_train[train_idx]
                    task_X_test = X_test[test_idx]
                    task_y_test = y_test[test_idx]
                    
                    class_type = class_types[i % len(class_types)]
                    class_factor = i * 0.2
                    
                    # Class-specific transformations
                    noise_level = 0.1 if class_type in ['textured', 'complex'] else 0.05
                    blur_factor = 0.1 if class_type in ['smooth', 'simple'] else 0.05
                    
                    train_dataset = GANTaskDataset(
                        images=task_X_train,
                        labels=task_y_train,
                        task_id=i,
                        domain_factor=class_factor,
                        noise_level=noise_level,
                        blur_factor=blur_factor
                    )
                    
                    test_dataset = GANTaskDataset(
                        images=task_X_test,
                        labels=task_y_test,
                        task_id=i,
                        domain_factor=class_factor,
                        noise_level=noise_level,
                        blur_factor=blur_factor
                    )

                    batch_size = self.config.get('batch_size', 32)
                    train_loader = DataLoader(
                        train_dataset, 
                        batch_size=batch_size, 
                        shuffle=True,
                        num_workers=self.config.get('num_workers', 0)
                    )
                    
                    test_loader = DataLoader(
                        test_dataset, 
                        batch_size=batch_size, 
                        shuffle=False,
                        num_workers=self.config.get('num_workers', 0)
                    )
                    
                    task_data = {
                        'task_id': i,
                        'X_train': task_X_train,
                        'y_train': task_y_train,
                        'X_test': task_X_test,
                        'y_test': task_y_test,
                        'train_dataset': train_dataset,
                        'test_dataset': test_dataset,
                        'train_loader': train_loader,
                        'test_loader': test_loader,
                        'class_type': class_type,
                        'class_factor': float(class_factor),
                        'description': f'Class {i+1}: {class_type} images',
                        'split_type': 'class'
                    }
                    
                    task_data['stats'] = {
                        'train_samples': len(task_X_train),
                        'test_samples': len(task_X_test),
                        'image_shape': task_X_train.shape[1:],
                        'class_type': class_type,
                        'class_factor': float(class_factor),
                        'train_mean': np.mean(task_X_train, axis=(0, 2, 3)).tolist() if task_X_train.ndim == 4 else np.mean(task_X_train, axis=0).tolist()
                    }
                    
                    tasks.append(task_data)
                
                return tasks

        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--train_loader', type=str, required=True)
            parser.add_argument('--test_loader', type=str, required=True)
            parser.add_argument('--config', type=str, required=True)
            parser.add_argument('--tasks', type=str, required=True)
            args = parser.parse_args()

            config = json.loads(args.config)

            # Load data
            with open(args.train_loader, 'rb') as f:
                train_data = pickle.load(f)
            
            if isinstance(train_data, dict) and 'loader' in train_data:
                train_loader = train_data['loader']
                print("Loaded training data with metadata")
            else:
                train_loader = train_data
                print("Loaded training data (legacy format)")
            
            with open(args.test_loader, 'rb') as f:
                test_data = pickle.load(f)
                
            if isinstance(test_data, dict) and 'loader' in test_data:
                test_loader = test_data['loader']
                print("Loaded test data with metadata")
            else:
                test_loader = test_data
                print("Loaded test data (legacy format)")

            # Extract tensors from DataLoaders
            # For GAN, we need both images and labels
            try:
                # Try to get both images and labels
                X_train = train_loader.dataset.tensors[0].numpy()
                y_train = train_loader.dataset.tensors[1].numpy() if len(train_loader.dataset.tensors) > 1 else np.zeros(len(X_train))
                X_test = test_loader.dataset.tensors[0].numpy()
                y_test = test_loader.dataset.tensors[1].numpy() if len(test_loader.dataset.tensors) > 1 else np.zeros(len(X_test))
            except:
                # Fallback: create dummy labels
                X_train = train_loader.dataset.tensors[0].numpy()
                y_train = np.zeros(len(X_train))
                X_test = test_loader.dataset.tensors[0].numpy()
                y_test = np.zeros(len(X_test))

            print(f"Training data shape: {X_train.shape}")
            print(f"Training labels shape: {y_train.shape}")
            print(f"Test data shape: {X_test.shape}")
            print(f"Test labels shape: {y_test.shape}")

            data = {
                'X_train': X_train,
                'y_train': y_train,
                'X_test': X_test,
                'y_test': y_test
            }
            
            # Create continual learning tasks
            strategy = config.get('cl_strategy', 'temporal_split')
            num_tasks = config.get('num_tasks', 3)
            
            print(f"Creating {num_tasks} continual learning tasks for Vanilla GAN using '{strategy}' strategy")
            
            splitter = GANTemporalDataSplitter(data, config, strategy=strategy)
            tasks = splitter.create_continual_tasks(num_tasks=num_tasks)

            # Add global task information
            task_summary = {
                'total_tasks': len(tasks),
                'strategy': strategy,
                'model_type': 'vanilla_gan',
                'original_train_size': len(X_train),
                'original_test_size': len(X_test),
                'data_shape': X_train.shape[1:],
                'config': {
                    'batch_size': config.get('batch_size', 32),
                    'cl_strategy': strategy,
                    'num_tasks': num_tasks,
                    'gan_type': 'vanilla_gan'
                },
                'tasks': tasks
            }

            # Print task summary
            print("\\n=== Vanilla GAN Continual Learning Task Summary ===")
            print(f"Strategy: {strategy}")
            print(f"Total tasks: {len(tasks)}")
            print(f"Data shape: {X_train.shape[1:]}")
            for i, task in enumerate(tasks):
                print(f"Task {i}: {task['description']} - Train: {task['stats']['train_samples']}, Test: {task['stats']['test_samples']}")

            os.makedirs(os.path.dirname(args.tasks), exist_ok=True)
            with open(args.tasks, "wb") as f:
                pickle.dump(task_summary, f)

            print(f"\nSaved continual learning tasks to {args.tasks}")

        if __name__ == '__main__':
            main()
    args:
    - --train_loader
    - {inputPath: train_loader}
    - --test_loader
    - {inputPath: test_loader}
    - --config
    - {inputValue: config}
    - --tasks
    - {outputPath: tasks}
